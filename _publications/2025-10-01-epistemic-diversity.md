---
title: "Epistemic Diversity and Knowledge Collapse in Large Language Models"
authors: "<strong>Dustin Wright</strong>, Sarah Masud, Jared Moore, Srishti Yadav, Maria Antoniak, Peter Ebert Christensen, Chan Young Park, Isabelle Augenstein"
collection: publications
permalink: /publication/2025-01-10-epistemic-diversity
excerpt: 'We assess the risk of knowledge collapse with LLMs by measuring epistemic diversity across 27 models in multiple settings.'
date: 2025-01-10
venue: 'arXiv'
paperurl: 'https://arxiv.org/abs/2510.04226'
bibtex: '@article{wright2025epistemic,
  title={{Epistemic Diversity and Knowledge Collapse in Large Language Models}},
  author={Wright, Dustin and Masud, Sarah and Moore, Jared and Yadav, Srishti and Antoniak, Maria and Christensen, Peter Ebert and Park, Chan Young and Augenstein, Isabelle},
  journal={arXiv preprint arXiv:2510.04226},
  year={2025}
}'
---
Large language models (LLMs) tend to generate homogenous texts, which may impact the diversity of knowledge generated across different outputs. Given their potential to replace existing forms of knowledge acquisition, this poses a risk of knowledge collapse, where homogenous LLMs may lead most people to be exposed to largely the same information, thus mediating a shrinking in the range of accessible information over time as underepresented knowledge is forgotten. To assess the risk of knowledge collapse with LLMs, we present a new methodology to measure epistemic diversity, i.e., variation in real-world claims in LLM outputs. We use this to perform a broad empirical study testing 27 LLMs, 155 topics covering 12 countries, and 200 prompt templates sourced from real user chats. For the topics in our study, we show that while newer models tend to generate more diverse claims, all models are less epistemically diverse than a basic web search. We find that model size has a negative impact on epistemic diversity, while retrieval-augmented generation (RAG) has a positive impact, though the improvement from RAG varies by the cultural context. Finally, compared to a traditional knowledge source (Wikipedia), we find that country-specific claims reflect the English language more than the local one, highlighting a gap in epistemic representation.

[Download paper here](https://arxiv.org/abs/2510.04226)

[Code and data](https://github.com/dwright37/llm-knowledge)


Recommended bibtex: 

```
@article{wright2025epistemic,
  title={{Epistemic Diversity and Knowledge Collapse in Large Language Models}},
  author={Wright, Dustin and Masud, Sarah and Moore, Jared and Yadav, Srishti and Antoniak, Maria and Christensen, Peter Ebert and Park, Chan Young and Augenstein, Isabelle},
  journal={arXiv preprint arXiv:2510.04226},
  year={2025}
}
```
